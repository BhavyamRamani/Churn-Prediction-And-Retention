              precision    recall  f1-score   support

           0       0.85      0.91      0.88      1036
           1       0.68      0.56      0.61       373

    accuracy                           0.81      1409
   macro avg       0.77      0.73      0.75      1409
weighted avg       0.81      0.81      0.81      1409

ðŸ’¾ Saved logistic_v1.pkl

ðŸš€ Training RF...
              precision    recall  f1-score   support

           0       0.83      0.91      0.87      1036
           1       0.65      0.49      0.56       373

    accuracy                           0.79      1409
   macro avg       0.74      0.70      0.71      1409
weighted avg       0.78      0.79      0.78      1409

ðŸ’¾ Saved rf_v1.pkl

ðŸš€ Training SVM...
              precision    recall  f1-score   support

           0       0.83      0.92      0.87      1036
           1       0.67      0.48      0.56       373

    accuracy                           0.80      1409
   macro avg       0.75      0.70      0.71      1409
weighted avg       0.79      0.80      0.79      1409

  bst.update(dtrain, iteration=i, fobj=obj)
              precision    recall  f1-score   support

           0       0.84      0.87      0.86      1036
           1       0.61      0.55      0.58       373

    accuracy                           0.79      1409
   macro avg       0.72      0.71      0.72      1409
weighted avg       0.78      0.79      0.78      1409

ðŸ’¾ Saved xgb_v1.pkl

ðŸ† Best Model: LOGISTIC with acc=0.8148
ðŸ’¾ Saved best_model.pkl




âœ… Training complete. Results: {'logistic': 0.8147622427253371, 'rf': 0.794889992902768, 'svm': 0.7991483321504613, 'xgb': 0.7863733144073811}



âœ… Final Comparison (v2 tuned models):
Logistic_v2: {'accuracy': 0.7494677075940384, 'precision': 0.5168918918918919, 'recall': 0.8203753351206434, 'f1': 0.6341968911917099}
RandomForest_v2: {'accuracy': 0.7835344215755855, 'precision': 0.5867346938775511, 'recall': 0.6166219839142091, 'f1': 0.6013071895424836}
XGBoost_v2: {'accuracy': 0.7913413768630234, 'precision': 0.5920745920745921, 'recall': 0.6809651474530831, 'f1': 0.6334164588528678}




V3 OUTPUT:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
ðŸ”¹ XGBoost Best Params: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}
              precision    recall  f1-score   support

           0       0.90      0.80      0.85      1036
           1       0.58      0.75      0.65       373

    accuracy                           0.79      1409
   macro avg       0.74      0.78      0.75      1409
weighted avg       0.81      0.79      0.80      1409


âœ… Final Tuned Model Parameters (v3):
Logistic_v3 : {'C': 10, 'max_iter': 500, 'penalty': 'l1', 'solver': 'liblinear'}
RF_v3 : {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}      
XGB_v3 : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}

ðŸ”¹ Logistic Regression Best Params: {'C': 10, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}
              precision    recall  f1-score   support

           0       0.92      0.72      0.81      1036
           1       0.52      0.82      0.63       373

    accuracy                           0.75      1409
   macro avg       0.72      0.77      0.72      1409
weighted avg       0.81      0.75      0.76      1409

ðŸ”¹ Random Forest Best Params: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}
              precision    recall  f1-score   support

           0       0.86      0.84      0.85      1036
           1       0.57      0.61      0.59       373

    accuracy                           0.78      1409
   macro avg       0.71      0.72      0.72      1409
weighted avg       0.78      0.78      0.78      1409


âœ… Final Tuned Model Parameters (v3):
Logistic_v3 : {'C': 10, 'max_iter': 500, 'penalty': 'l1', 'solver': 'liblinear'}
RF_v3 : {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}  